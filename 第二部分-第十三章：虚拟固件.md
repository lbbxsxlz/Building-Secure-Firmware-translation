## 第十三章

# 虚拟固件

在前几章中，我们讨论了系统中真实固件的安全设计。现在我们来看一下虚拟固件。图13-1是一个典型的I型虚拟化架构。当系统固件完成平台初始化后，会启动管理程序。然后，管理程序创建四个域并启动它们。每个客户机域都有自己的虚拟固件。虚拟固件为客户机操作系统准备所需的接口，并启动客户机操作系统。

在真实机器中，固件的目的是初始化硬件，并为操作系统（OS）提供通用接口。因此，操作系统可以与平台无关。在虚拟机中，虚拟固件也有同样的作用。虚拟固件为客户机操作系统提供相同的通用接口。例如，如果客户机操作系统是UEFI Windows或UEFI Linux，虚拟固件需要为客户机UEFI操作系统创建UEFI环境。

<div align=center><img src=Figures/Chapter-13-Screenshot/Figure-13-1.jpg></img></div>
<div align=center>图 13-1 虚拟化架构</div>

## 客户机域中的新威胁

当我们谈到安全，首先得要了解威胁模型。基于两种不同的使用案例，虚拟固件有两种不同的威胁模型：

    1） 客户机域信任管理程序

1972年，Anderson在“计算机安全技术规划研究”技术报告中提到了“参考监控器”的概念。参考监控器是一个抽象的机器，它控制着主体对客体的访问。参考监控器确保主体对客体拥有访问权限，并保护客体免受未经授权的访问。1974年，Popek和Goldberg发表了“可虚拟化的第三代体系结构的形式要求”，并提到了虚拟机监控器（VMM）（也称为管理程序）的三个特性：1）效率；2）资源控制；3）等价性。由于VMM可以控制系统资源，因此它是个很好的参考监控器备选。在大多数当前虚拟化架构中，VMM可以访问和控制客户机域的行为。例如，IT人员可能会在发送给企业用户的笔记本电脑上安装一个管理程序来保护客户机操作系统。当最终用户使用操作系统时，他们会信任管理程序。

在这种情况下，客户机依赖管理程序提供保护。虚拟固件可以信任硬件设备和管理程序提供的软件服务。管理程序控制着一切。虚拟固件无需考虑特殊的新威胁。

    2） 客户机域不信任管理程序

然而，云世界的情况发生了变化。在基础设施即服务（IaaS）模式中，云服务提供商（CSP）在物理机上建立了虚拟机监控器，并提供虚拟机作为服务来出租。现在的问题是：租客是否信任云服务提供商？让我们逐一考虑CIA的三要素：机密性、完整性和可用性。首先是机密性。租客可能不想与CSP共享公司的秘密数据。其次是完整性。租客可能不想让CSP修改任何公司数据。第三是可用性。租客可能不希望来自CSP的服务意外中断。

可用性应该由CSP维护。CSP在提供云服务时应采用可靠性、可用性和可维护性（RAS）。非信任输入在使用前应经过验证，例如网络数据包或磁盘镜像，这与真实系统固件类似。但是，如果CSP因网络断开或管理程序关闭而停止服务，虚拟固件将无能为力。

这种情况下的保密性和完整性比较特殊，因为CSP被视为恶意，并且CSP可能会尝试攻击客户机域。例如，攻击者可能会在管理程序中放置扫描仪来扫描客户机内存以获取机密。攻击者可能会在虚拟固件中植入rootkit来控制客户机操作系统。因此，客户机域需要一种方法来保护其数据免遭管理程序的攻击。

从高层来看，虚拟机数据可分为三类：1）处于静态的数据；2）传输中的数据；3）使用中数据。处于静态的数据可以通过加密来保护。传输中的数据可以通过网络协议（例如传输层安全TLS）来保护。使用中数据的保护需要虚拟固件的硬件辅助。

### 案例研究

现在，让我们来看一下虚拟化中安全扩展的一些实际案例。

#### AMD安全加密虚拟化（SEV）

当CPU执行代码或引用数据时，需要访问明文内存。在目前的大多数架构中，DRAM中的数据也是明文。这就带来了潜在的风险，如果攻击者能劫持内存总线，他们就能读取数据并修改数据。为了减少这种攻击，AMD采用了安全内存加密（SME），对发送到DRAM的数据进行加密（见图13-2）。因此，内存总线上或DRAM中的数据都是密文。每个CPU片上系统（SOC）都包含一个带有一个高级加密标准（AES）引擎AMD安全处理器（AMD-SP），用于加密和解密数据。AES密钥在每次重置系统时随机生成，并且SOC外部无法使用。

SME功能可以根据页表来部分启用。页表中的物理地址有一个enCrypted位（C位），用于指示该页是否需要加密。因此，操作系统可以决定哪些内存范围需要加密。为了向不知道新C位的操作系统提供支持，CPU可以代替使用透明SME(TSME)。在TSME模式下，无论C位是设置还是清除，所有内存都会被加密。

<div align=center><img src=Figures/Chapter-13-Screenshot/Figure-13-2.jpg></img></div>
<div align=center>图 13-2 AMD SME架构</div>

SME功能很好，但并不能解决云世界中的所有问题，因为管理程序仍然可以访问客户机域的内存。只有一个密钥在SME中使用。

AMD安全加密虚拟化（SEV）技术使用SME，并增加了一个具有多个加密密钥的扩展功能（见图13-3）。在启动过程中，每个客户机域都有一个唯一的虚拟机（VM）地址空间ID（ASID）。SEV硬件为数据或代码分配一个了带有虚拟机ASID的标签。当数据进入或离开SOC时，数据将根据相关标签使用唯一密钥进行加密或解密。因此，管理程序或其他域无法访问该域中的内存。

<div align=center><img src=Figures/Chapter-13-Screenshot/Figure-13-3.jpg></img></div>
<div align=center>图 13-3 AMD SEV架构</div>

在某些情况下，客户机域需要与管理程序通信来共享信息。因此，客户机可以选择禁用通信缓冲区的加密，将其作为共享页面，而将其他页面加密为私有页面（见图13-4）。

<div align=center><img src=Figures/Chapter-13-Screenshot/Figure-13-4.jpg></img></div>
<div align=center>图 13-4 AMD SEV通信</div>

共享内存的一个例子是用于直接内存访问（DMA）的内存。DMA不允许加密。如果客户机设备驱动程序需要使用DMA与管理程序模拟的设备或真实硬件设备通信，设备驱动程序需要分配一个DMA缓冲区并清除C位。当DMA缓冲区被回收为正常内存页时，该驱动程序需要清除C位，将其标记为私有页。

SEV的安全性基于加密密钥的安全性。由于每个虚拟机都需要一个唯一的密钥，SEV固件需要提供一个密钥管理接口，来执行三个安全属性： 1）平台的真实性；2）已启动客户机的证明；3）客户机数据的机密性。

平台的真实性可由SEV固件中的身份密钥证明。每个SEV平台都包含一个由AMD和平台所有者分配的身份密钥。客户机的证明可通过客户机的镜像测量来证明。当SEV固件启动客户机固件时，它也会测量客户机镜像。之后，SEV固件可将测量提供给客户机的所有者。由于SEV固件已通过身份密钥认证，因此客户机所有者会信任SEV固件。只有在客户机所有者验证了测量并决定信任该客户机镜像后，客户机所有者才会向客户机镜像传输更多数据，例如用于解密虚拟机磁盘的磁盘加密密钥。客户机数据的保密性基于SEV固件已知的内存加密密钥。SEV固件不得泄露加密密钥，也不得泄露任何有关加密密钥的线索。

AMD SEV可对客户机内存进行加密。但管理程序可能会通过修改RAX/RBX/RCX/RDX等客户寄存器来篡改客户程序的执行。AMD SEV加密状态（SEV-ES）为客户机寄存器状态提供了额外保护。在启动过程中，客户机寄存器被初始化为已知状态，并作为SEV启动过程的一部分进行加密和测量。完整性检查针对每个VMRUN进行。使用SEV-ES为虚拟机分配专用的虚拟机保存区（VMSA），其中包含例如段状态、控制状态、通用寄存器（GPR）状态和浮点运算单元（FPU）状态等信息。
(FPU) 状态等信息。

如果客户机域需要使用GPR与管理程序通信，那么管理程序就必须访问客户机寄存器区域。因此，客户机需要将寄存器复制到专用的客户机管理程序通信块（GHCB）。见图13-5。

<div align=center><img src=Figures/Chapter-13-Screenshot/Figure-13-5.jpg></img></div>
<div align=center>图 13-5 使用GHCBAMD SEV-ES通信</div>

让我们以CPUID指令为例。CPUID指令使用EAX寄存器作为输入，并使用EAX、EBX、ECX和EDX寄存器作为输出。在没有SEV-ES的虚拟化环境中，当客户机域执行CPUID指令时，会触发VmExit。管理程序会解析VmExit原因，来了解是CPUID指令触发了VmExit。然后，管理程序会获取EAX值作为CPUID索引，并在客户机通用寄存器字段中设置EAX/EBX/ECX/EDX，然后恢复客户机。

然而，这一流程无法与SEV-ES配合使用，因为管理程序不被允许直接从客户机读取EAX或向客户机写入EAX/EBX/ECD/EDX。有两种方法可以解决这个问题：1）客户机软件可以用另一种操作取代CPUID指令，即写入GHCB中的EAX、触发VmExit，并从GHCB中读取EAX/EBX/ECX/EDX。这会给现有的客户机软件二进制文件带来兼容性问题。2）另一种方法是利用新的VMM通信异常（#VC）。见图13-6。当客户机软件执行CPUID时，CPU硬件会触发一个带有错误码的#VC异常，以指示该指令是什么。然后，客户机#VC异常处理程序可将指令（CPUID）及其输入参数（EAX）写入GHCB，并触发VmExit。相应的管理处理程序会解析GHCB中的指令和输入数据，将指令的输出数据写入GHCB，并恢复到客户机。然后，客户机#VC处理程序将指令（CPUID）的输出参数（EAX/EBX/ECX/EDX）读到CPU寄存器。在新的#VC处理程序的帮助下，兼容性得以保持。

<div align=center><img src=Figures/Chapter-13-Screenshot/Figure-13-6.jpg></img></div>
<div align=center>图 13-6 使用GHCB和#VC的AMD SEV-ES通信</div>

这种GHCB通信机制可用于需要管理程序解析客体机状态的指令，例如CPUID、读MSR、写MSR、I/O读、I/O写等。有了SEV-ES，客户机内存和客户机状态就不会被管理程序篡改。客户机固件需要启用该功能，以提供其执行环境。

更多详细信息，请参阅《AMD架构程序员手册》。

#### 英特尔可信域执行（TDX）

英特尔提供内存加密功能。全内存加密（TME）提供基本的内存加密功能。它类似于AMD SME。它使用一个短暂密钥对系统的整个物理内存进行加密。为了支持虚拟化环境中的客户机内存安全性，英特尔以灵活的方式提供了多密钥全内存加密（MKTME）。通过MKTME，每个页面都可以标记一个密钥ID。在运行时期间，管理程序会为不同的虚拟机分配不同的KeyID，以实现域隔离。以图13-7为例。管理程序有一个共享KeyID 0和一个私有KeyID 1。客户机域0与管理程序共享KeyID 0、与域1共享的KeyID 4和私有KeyID 2。客户机域1与管理程序共享KeyID 0，与域1共享KeyID 4，以及私有KeyID 3。

<div align=center><img src=Figures/Chapter-13-Screenshot/Figure-13-7.jpg></img></div>
<div align=center>图 13-7 英特尔使用KeyID的MKTME通信</div>

对于DMA，管理程序可以在IOMMU页表中设置KeyID，类似于在外部页表（EPT）——主机页表中设置KeyID的方式。对于物理DMA，KeyID可直接应用于物理地址。这种设计可以简化客户机软件。

更多详细信息，请参阅英特尔MKTME规范。

在英特尔MKTME和虚拟化技术的基础上，英特尔推出了可信域执行（TDX）技术。英特尔TDX类似于AMD SEV。它可以提供一个硬件隔离的虚拟机（VM），称为可信域（TD）。TD有两种功能：

    1）内存和CPU状态的机密性和完整性。TD只信任：在安全仲裁模式（SEAM）下执行的英特尔TDX模块、英特尔认证代码模块（ACM），也称为SEAM加载程序（SEAMLDR）、英特尔TD Quoting Enclave(QE)和英特尔CPU硬件。TD中的数据受到保护，可抵御来自软件的攻击，例如其他虚拟机或TD、管理程序、BIOS系统管理模式（SMM）、集成设备等。它还能抵御来自硬件的攻击，例如离线内存分析和主动内存攻击，包括捕获、修改、重定位、拼接和别名。但是，内存重放攻击不在此范围内。TD仍依赖管理程序来设置资源。内存和CPU状态的可用性不在攻击范围内。

    TDX模块是英特尔TDX的一个关键概念。它由SEAM加载程序加载到SEAM内存中，并受SEAM范围寄存器（SEAMRR）的保护。它管理TD与管理程序之间的转换。如果出现虚拟机退出事件导致TD客户机退出，CPU将切换到英特尔TDX模块，而不是管理程序。然后，英特尔TDX模块使用SEAMRET指令切换到管理程序。管理程序处理虚拟机退出事件后，使用SEAMCALL指令切换到英特尔TDX模块。然后，英特尔TDX模块执行虚拟机进入，以恢复TD客户机。图13-8显示了英特尔TDX模块流程。

<div align=center><img src=Figures/Chapter-13-Screenshot/Figure-13-8.jpg></img></div>
<div align=center>图 13-8 英特尔TDX模块流程</div>

    图13-9显示了英特尔TDX内存保护是如何工作的。TD可以访问两类内存 —— 私有内存，包含TD内部的机密数据；共享内存，用于与TD外部不受信任的代理进行通信，例如管理程序服务或设备驱动程序的DMA或I/O。使用TDX时，客户机物理地址（GPA）中的最高位被指定为“共享”位。专用内存的共享位为0，共享内存的共享位为1。当TD执行时，有两个不同的扩展页表（EPT）—— 共享EPT由管理程序设置，使用共享keyID管理共享GPA转换；安全EPT由英特尔TDX模块设置，使用私有keyID管理私有GPA转换。

    除了安全的EPT外，英特尔TDX模块还维护CPU状态的机密性和完整性。当创建TD时，英特尔TDX模块需要额外的内存来设置虚拟机控制状态（VMCS）和TD状态保存区域等。这些数据结构受TD私钥保护。因此，管理程序无法篡改它们。

<div align=center><img src=Figures/Chapter-13-Screenshot/Figure-13-9.jpg></img></div>
<div align=center>图 13-9 英特尔TDX内存保护</div>

    2）远程证明允许TD的所有者和TD中服务的消费者确定他们所依赖的TD中可信计算基础（TCB）的版本，以此来保护数据。它使人们确信，服务运行在真正的英特尔TDX系统上的TD内部。

在TD创建期间，管理程序会分私有内存，并要求英特尔TDX模块加载初始内存内容。英特尔TDX模块还会将初始内存内容及其元数据，例如内存地址，扩展到TD测量寄存器（TDMR）中。英特尔TDX模块启动TD后，初始TD可以继续将更多信息。例如附加代码和配置数据，扩展到一组运行时可扩展测量寄存器（RTMR）中。TDMR和RTMR都用于TD的远程证明。

图13-10显示了远程证明架构流程。

步骤 1：质疑者向TD发送质疑请求。

步骤 2：TD要求英特尔TDX模块为TD获取报告。

步骤 3：英特尔TDX模块调用SEAMREPORT指令。

步骤 4：TDX CPU硬件生成带有信息认证码（MAC）的报告，并将其返回给英特尔TDX模块。报告包括安全版本号（SVN），例如CPU SVN和SEAMLDR SVN，以及TD信息的哈希值。TD信息包括测量寄存器，例如TDMR和RTMR，和TD属性，例如TD所有者（MROWNER）。

步骤 5：英特尔TDX模块从硬件获取报告，合并TD信息，并将完整报告报告给TD。

步骤 6：现在，TD获得了报告。但它很难使用，因为它只使用MAC来保证完整性。TD需要将报告转换为引述（quote），用于证明。因此，TD将报告发送给管理程序，并对报告请求引述。

步骤 7：管理程序找到一个TD的引述飞地 (quote enclave, QE)，并将引述请求与TD报告一起发送。

步骤 8：当TD的引述飞地收到TD报告时，它会使用EVERIFYREPORT指令让CPU验证报告的完整性。

步骤 9：如果报告的MAC正确，则表示报告是真实的。CPU硬件将信息返回给引述飞地。

步骤 10 然后，TD的引述飞地使用证明密钥对报告签名，并将引述返回给管理程序。

验证密钥可由验证基础设施验证。英特尔配置认证飞地（PCE）旨在充当本地TD引述飞地的本地证书颁发机构。在TD的引述飞地启动期间，它会生成自己的认证密钥并提交给PCE。然后，PCE认证请求，并为TD引述飞地颁发证书类似结构和证明密钥。该结构由配置认证密钥 (PCK) 签名，这由英特尔维护。

步骤 11：管理程序将引述返回给TD。

步骤 12：TD现在回复质疑，并将TD的引述返回给质疑者。

步骤 13：质疑者可使用证明密钥证书验证引述，使用英特尔提供的证书密钥（PCK）验证证明密钥证书。如果验证通过，说明引述是真实的。

步骤 14：质疑者可解析TD报告并获取TD信息，例如TD测量（TDMR和RTMR），来查看它们是否符合预期。

<div align=center><img src=Figures/Chapter-13-Screenshot/Figure-13-10.jpg></img></div>
<div align=center>图 13-10 英特尔TDX远程证明</div>

#### IBM安全虚拟机（SVM）

与AMD安全加密虚拟化（SEV）类似，IBM在IBM Power Systems上启用了安全虚拟机（SVM）。一种新模式，—— ultravisor模式 —— 被引入来支持SVM。ultravisor模式的权限高于hypervisor模式。内存被划分为安全内存和普通内存。作为安全启动过程的一部分，保护执行超级管理程序（或Ultravisor）被加载到安全内存中。当SVM启动时，SVM中的软件会调用Ultravisor将SVM移至安全内存，并在安全内存中继续执行。SVM使用管理程序调用（h-calls）从管理程序获取服务，并使用超级管理程序调用（u-calls）到超级管理程序。Ultravisor会过滤SVM与管理程序之间的所有调用，以确保只有执行调用所需的信息才会传递给管理程序。它还能确保只有来自管理程序的响应才能返回给SVM。硬件可防止未经授权的系统软件和硬件引用安全内存。

SVM只能在支持IBM保护执行机制（PEF）的系统上运行。IBM PEF有一对公钥/私钥。私钥只有系统知道（不会暴露给系统所有者），并且只有在启动了正确、经过验证的固件后才能使用。在创建安全虚拟机时，目标系统的公钥被用来封装安全虚拟机的加密密钥。

Ultravisor会向管理程序提供SVM内存页面的加密版本。用于保护SVM的加密模式是高级加密标准（AES）的扩展，称为“完整性感知可并行模式”（IAPM），它可以检测管理程序对SVM页面加密表达的任何修改。对于设备I/O，使用虚拟I/O反弹缓冲区。SVM需要向Ultravisor说明，其内存的特定页面将在不受保护的情况下与管理程序共享。

#### EDK II支持

EDK II实现了开放虚拟机固件（OVMF）。为了支持SEV，OVMF为私有内存设置了带有C位的客户机页表。当客户机需要DMA时，OVMF代码需要为共享DMA缓冲区清除C位，并在DMA缓冲区被回收为私有内存时再次设置C位。为了支持SEV-ES，OVMF实现了#VC处理程序，以便将特殊指令与GHCB访问进行转换。

### 攻击与缓解

图13-11显示了一个基础设施即服务 (IaaS) 使用案例。租户向云服务提供商（CSP）提供包含操作系统和应用数据的客户机磁盘。然后，CSP为租户分配内存，并使用管理程序启动客户机磁盘镜像。现在让我们假设CSP/管理程序是恶意的，并看看攻击点。

<div align=center><img src=Figures/Chapter-13-Screenshot/Figure-13-11.jpg></img></div>
<div align=center>图 13-11 一个IaaS使用案例</div>

#### 攻击静态客户机数据

静态数据是客户磁盘镜像。管理程序可能读取磁盘上的数据或篡改数据。至少有两种可能的保护方式：
1) 加密：如果全磁盘镜像已加密，那么管理程序就无法在没有密钥的情况下读取内容。如果管理程序修改了磁盘镜像，解密后的数据将无效。
2) 测量：磁盘镜像加载到内存后，租户会使用硬件辅助机制来测量磁盘镜像。测量的局限性在于无法阻止管理程序读取磁盘内容。在内存中测量整个镜像的过程比较复杂，而且还需要额外的信任根来支持测量。

因此，我们假设磁盘选择了加密解决方案。现在的问题是由谁来解密，以及如何传输磁盘加密密钥。

除了磁盘镜像，租户还可以选择向管理程序传输客户机加载程序。客户机加载程序非常简单，没有任何秘密。因此，客户加载程序可以是明文的。客户机加载程序有办法与租户服务器通信，获取磁盘加密密钥，用来解密磁盘。由于磁盘加载程序简单且没有秘密，因此可以对磁盘加载程序使用测量和验证机制。在启动过程中，测量信任根（例如SEV固件）可以测量磁盘加载程序。之后，租户服务器可对客户加载程序使用远程证明，以确保客户加载程序未被管理程序篡改。

#### 攻击过渡中的客户机数据

过渡中数据是磁盘加密密钥。密钥应从租户密钥服务器传输到客户加载程序，来解密客户机磁盘映像。这是一种典型的网络安全用法。传输层安全（TLS）来可用在这确保密钥受到保护。在租户服务器和客户加载程序创建会话来传输密钥之前，必须完成相互认证，来防止中间人攻击。客户机加载程序可以在TLS握手阶段加入服务器公共证书，来验证密钥服务器。租户密钥服务器可使用远程证明结果作为客户机加载程序未被篡改的证据。

#### 攻击使用中的客户机数据

为了保护使用中的数据，必须使用内存加密。SEV或MKTME都可在此使用。因此，管理程序就无法访问客户机内存。把它们结合在一起，解决方案如图13-12所示。假设租户信任自己和独立硬件供应商 (IHV)，租户不信任云服务提供商 (CSP)。

<div align=center><img src=Figures/Chapter-13-Screenshot/Figure-13-12.jpg></img></div>
<div align=center>图 13-12 一个IaaS使用案例 —— 受保护的客户机域</div>

步骤 1：租户服务器向CSP管理程序传输客户机加载程序和客户机磁盘镜像。

步骤 2：CSP管理程序为租户创建客户机环境并启动客户加载程序。硬件组件参与这一启动过程。硬件会测量客户机负载，并为客户机内存启用内存加密。

步骤 3：客户机加载程序与租户服务器通信，获取磁盘加密密钥。在租户密钥服务器传输密钥之前，密钥服务器使用远程证明与硬件通信，以确保客户加载程序未被修改。

步骤 4：客户加载程序获取密钥，解密客户磁盘镜像并加载。

#### 攻击硬件信任根

在前述过程中，硬件信任根是一个关键组件。它维护客户机的内存加密密钥，并为客户域提供测量。如果硬件信任根被破坏，前面的保护就不会生效。

缺乏完整性保护是内存加密技术的一个弱点。研究人员已经展示了SEVered攻击，该攻击通过将整个内存区域以明文形式重映射到虚拟机中的任意页面，提来取受AMD SEV保护的虚拟机数据。

如果硬件提供密钥管理服务，这个接口就是一个很好的攻击点。例如，旧的AMD SEV椭圆曲线（ECC）实现容易受到无效曲线攻击，导致平台Diffie-Hellman(PDH)私钥泄露。利用PDH，攻击者可以恢复整个会话密钥和虚拟机启动密钥。

密钥碰撞是基于多密钥的内存加密技术的另一个潜在风险。如果硬件只支持有限的密钥数或使用弱随机化来推导密钥，而管理程序又同时启动多个客户机域，那么两个虚拟机使用相同加密密钥的几率可能会非常小。

## 设备接口

客户机域运行在管理程序模拟的硬件之上。虚拟设备接口因实现方式而异，例如：
1) 仿真硬件
虚拟硬件可以具有与真实硬件相同的接口，虚拟硬件由管理程序完全仿真。这种设备应处于完全虚拟化环境中。
2) 合成硬件
虚拟硬件可以是管理程序专用的标准接口，例如使用Linux内核虚拟机（KVM）和Quick Emulator（QEMU）的虚拟I/O设备（virtio），以及使用Microsoft Hyper-V的虚拟机总线（vmbus）。该设备应在准虚拟化环境中使用，以获得更好的性能。
3) 物理硬件（直通）
虚拟硬件可以是管理程序在直通模式下分配的物理设备，例如图形设备。这可以通过硬件分区来支持。管理程序只让客户机在IOMMU的帮助下管理设备。
4) 物理硬件的虚拟设备接口（直通）
虚拟硬件可以是物理设备上的虚拟设备接口，也可以是物理设备的直通接口，例如PCI Express单根I/O虚拟化（SR-IOV）和英特尔可扩展I/O虚拟化。

### 案例研究

现在，让我们看一下虚拟化中设备接口支持的一些实际案例。

#### 单根I/O虚拟化（SR-IOV）

SR-IOV设备可以有一个物理功能（PF）和多个虚拟功能（VF）。见图 13-13。虚拟功能是一种轻量级PCI Express功能，被分配唯一的路由ID。虚拟功能有自己专用的PCI配置空间、基地址寄存器（BAR）、存储映射I/O（MMIO）资源、消息信号中断（MSI）存储表、功能级复位（FLR）、高级错误报告（AER）、电源管理（PM）等。虚拟功能必须拥有与物理功能相同的设备类型。每个虚拟功能可由主机VMM分配给一个专用的客户机域。因此，客户机域就可以直接控制和使用设备，而无需VMM的中断。这种设计可以提高性能，并解决物理设备的限制。

网络接口卡可实现SR-IOV来提高虚拟化环境中网络流量的吞吐量。非易失性存储器快速接口（NVMe）也在NVMe规范中增加了SR-IOV支持来实现灵活的资源管理。

<div align=center><img src=Figures/Chapter-13-Screenshot/Figure-13-13.jpg></img></div>
<div align=center>图 13-13 带有SR-IOV的虚拟功能分配</div>

#### 可扩展I/O虚拟化（可扩展IOV）

SR-IOV是通过复制每个虚拟功能的硬件/软件接口来实现，例如存储映射I/O（MMIO）资源和消息信号中断（MSI）。这种复制增加了设备的复杂性，并在扩展到大量虚拟功能时带来限制。为了解决这个问题，英特尔推出了可扩展I/O虚拟化（IOV）技术。

英特尔可扩展IOV设备将硬件/软件接口分为两类：快速路径和慢速路径。快速路径访问侧重于运行时的数据传输。慢路径访问侧重于初始化、配置、复位、错误处理等。有了这些类别，快路径访问直接映射到物理设备，而慢路径访问则由特定设备的主机软件模拟。

在SR-IOV中，快速路径类别是通过虚拟功能实现的。然而，真正需要的是专用的MMIO资源和中断资源。英特尔可扩展IOV对MMIO资源使用可分配设备接口（ADI），来代替虚拟功能中的MMIO基地址寄存器（BAR），对中断采用中断消息存储（IMS）来代替MSI-X表存储（见图 13-14）。ADI位于物理功能的MMIO BAR中，而不是虚拟功能的MMIO BAR中。所有ADI共享同一个物理BAR，并共享同一个请求者ID(RID) —— 物理功能的总线/设备/功能。每个ADI必须被分配一个进程地址空间标识符（PASID）。RID和PASID的区别在于，RID用于将事务路由到I/O结构，而PASID则用于传递内存事务的目标地址空间。ADI使用IMS进行中断存储。IMS与虚拟MSI-X类似，但它是在物理功能中实现的，而不是虚拟功能。

<div align=center><img src=Figures/Chapter-13-Screenshot/Figure-13-14.jpg></img></div>
<div align=center>图 13-14 带有SR-IOV的可分配设备接口</div>

这种方法的好处是，高性能I/O仍可通过快速路径（ADI和ISM）实现，而可扩展性则在慢速路径的设备模拟帮助下通过更简单的硬件实现。

#### 虚拟I/O设备（virtio）

有了完全虚拟化，管理程序必须拦截对硬件设备的所有命令，并模拟硬件行为，将响应发送回客户机。由于虚拟机退出耗费和仿真耗费，这会对性能造成影响。它还会带来硬件兼容性风险，因为管理程序必须模拟所有支持的硬件。半（准）虚拟化只需对客户机操作系统进行少量修改，简化了管理程序，因此能带来巨大的性能提升。使用半（准）虚拟化技术，客户机需要实现设备前端来消费功能，而管理程序需要实现设备后端来提供服务。

为了支持互操作性和兼容性，有必要在管理程序和客户机之间定义一个标准接口。接口只需要关注设备的功能，例如存储、网络、输入、输出等。

virtio是Linux侧的标准接口之一（见图13-15）。它定义了网络设备、块设备、控制台设备、熵设备、内存气球设备、SCSI主机设备、GPU 设备、输入设备、加密设备和套接字设备。virtio设备可以被客户机通过标准PCI总线、MMIO或通道I/O发现。

EDK II开放式虚拟机固件（OVMF）实现了virtio设备驱动程序，并支持通过virtio PCI接口传输。OVMF在共享缓冲区中准备数据，并发送 I/O请求，请求由基于Linux内核的虚拟机（KVM）截获。然后，KVM将virtio请求通知快速模拟器（QEMU）。正是QEMU实现了virtio后端，它解析共享缓冲区中的数据，并通过共享缓冲区发回响应。

<div align=center><img src=Figures/Chapter-13-Screenshot/Figure-13-15.jpg></img></div>
<div align=center>图 13-15 virtio设备</div>

#### 虚拟机总线（vmbus）

vmbus是Microsoft Hyper-V中的客户机管理程序通信通道（见图13-16）。客户机域可以有多个虚拟服务客户端（VSC），例如网络、视频、存储和人机接口设备（HID）。VSC可以使用vmbus与根分区中的虚拟服务提供商（VSP）直接通信，来提高虚拟化性能。

使用标准vmbus，Linux客户机可包含vmbus驱动程序（VSC），并在Microsoft Hyper-V环境中运行。反之亦然，Windows客户机可以使用 vmbus驱动程序，并在支持vmbus和VSP的Linux KVM/QEMU中运行。

<div align=center><img src=Figures/Chapter-13-Screenshot/Figure-13-16.jpg></img></div>
<div align=center>图 13-16 vmbus设备</div>

### 攻击与缓解

现在，让我们看一下来自虚拟设备或物理设备的攻击的一些实例与缓解措施。

#### 设备输入

无论设备是物理的（直接直通、SR-IOV或可扩展IOV）还是虚拟的（virtio或vmbus），都可以用来攻击客户机域。例如，设备标识符数据或 DMA输入缓冲区可提供虚假设备输入。恶意设备可能会将畸形数据填入数据缓冲区。

黑客已经展示了使用共享环形缓冲区VM逃逸，实施从客户机域到管理程序的攻击。管理程序可能会使用类似的技术来攻击客户机域。

与第8章讨论的内容类似，客户机设备驱动程序应该
1) 在平台信任根的帮助下验证设备并建立安全通信通道。
2) 仔细解析设备数据，拒绝任何无效数据。

为防止检查时间/使用时间（TOC/TOU）攻击，客户机驱动程序应在执行检查前将共享缓冲区复制到专用内存中。

#### 设备热插拔

虚拟机可能支持运行时资源更新，例如支持CPU和内存卡热插拔。我们在第8章讨论了CPU热插拔和内存卡热插拔功能及威胁。对物理机的威胁也可应用于虚拟机，例如对系统管理模式（SMM）的攻击。如果客户机域实现了SMM，则客户机应使用类似的保护机制来保护SMM。

#### 加密设备

在实际环境中，硬件可以实现加速加密功能。例如，ARMv8 CPU可执行对称加密（AES、SM4）、哈希（SHA、SHA2、SHA3、SM3）和随机化（RNDR）指令。英特尔CPU执行随机化（RDSEED、RDRAND）、哈希（SHA、SHA2）和对称加密（AES）指令。

在虚拟化世界，虚拟设备也可提供此类支持。例如，virtio提供返回随机数据的熵设备和提供加密服务的加密设备，如对称加密服务（ARC4、AES、3DES）、哈希服务（MD5、SHA1、SHA2、SHA3）、消息认证码（MAC）服务（HMAC、CMAC、GMAC、CBCMAC）和带关联数据的认证加密（AEAD）服务（GCM、CCM、ChaCha20-Poly1305）。

如果客户机将管理程序视为恶意软件，则这些虚拟加密设备就不应该使用。如果客户机信任为客户机提供保护的硬件，客户机可以选择使用受信任硬件提供的服务。

## 特殊功能

除了启动到客户机操作系统外，虚拟固件还可能实现一些特殊功能。每种功能都可能有自己的特殊威胁模型。

### 案例研究

现在，让我们看下安全功能的一些实例。

#### 可信启动

我们在第7章讨论了可信启动在真实世界中的实现。可信启动也可用于虚拟化环境，以证明客户机的启动流程未被修改。

在物理世界中，可信启动可依赖于可信平台模块（TPM），它提供一组平台配置寄存器（PCR）。这些寄存器充当报告的信任根（RTR），来记录系统状态并支持远程证明。TPM也提供了受保护的存储区域，作为密钥和数据的存储信任根（RTS）。TPM不能提供测量信任根（RTM）。RTM应该是主机侧的某些代码，由Core-RTM（CRTM）来测量平台重置后运行的组件。

客户机域可能有一个虚拟TPM(vTPM)来提供虚拟RTR(vRTR)和虚拟RTS(vRTS)，并有一个虚拟RTM(vRTM)来进行测量。vTPM和vRTM必须位于可信计算库（TCB）中。

如果客户机可以信任管理程序，则vTPM（vRTR和vRTS）和vRTM可由管理程序提供。图13-17显示了一种可能的实现方式。当管理程序启动虚拟固件时，vRTM可以测量虚拟固件，将结果写入到vTPM中。客户机域中的测量过程与物理环境中的过程相同。当远程质疑者希望使用远程证明来检查客户机环境是否可信时，质疑者可以使用标准远程证明流程来与客户机域中的验证服务通信，并获取vTPM PCR引述。为了验证vTPM的真实性，质疑者还需要对提供vTPM的管理程序使用远程证明。

<div align=center><img src=Figures/Chapter-13-Screenshot/Figure-13-17.jpg></img></div>
<div align=center>图 13-17 来自管理程序的虚拟TPM</div>

如果客户机无法信任管理程序，管理程序就无法提供vTPM（vRTR和vRTS）和vRTM。图13-18显示了另一种可能的实现方式。vRTR、vRTS和vRTM由片上系统（SOC）提供。当管理程序要启动客户机域时，SOC协助客户启动过程。vRTM将测量结果记录到SOC内部的vRTR中。客户机可使用SOC提供的测量服务来扩展客户机域中的数据。如本章第一节所讨论的，SOC必须保持客户机域与管理程序之间的隔离。因此，恶意管理程序无法篡改客户机执行环境和记录在SoC中客户机执行上下文。vRTR、vRTS和vRTM必须是域专用的，不能被管理程序或其他客户机域访问。当质疑者要验证客户机域时，首先要使用客户机的远程证明。然后，质疑者还需要对SOC进行远程证明，来确保SOC的真实性。

<div align=center><img src=Figures/Chapter-13-Screenshot/Figure-13-18.jpg></img></div>
<div align=center>图 13-18 来自SOC的虚拟TPM</div>

#### 安全启动

安全启动应用在一个组件加载下一个启动组件时。如果下一个组件来自不同的供应商，且不可信，安全启动可帮助验证下一个组件在加载前是否未经修改。在客户机域中也是如此。虚拟固件可以使用UEFI安全启动来确保只能加载经过验证的客户操作系统。这可以减轻对客户机操作系统的某些攻击。

与真实环境中的安全启动策略类似，客户机安全启动策略应存储在受保护的存储中，并且只能在可信的执行环境中更新。因此，其他客户机软件无法在未经授权的情况下更新安全启动策略。安全启动策略可由信任根更新，根据威胁模型，信任根可以是管理程序或SOC。如果客户机域信任管理程序，客户机固件就可以依靠管理程序来模拟闪存设备并在其中存储安全启动策略。客户机固件应锁定客户机闪存设备，这样其他客户机软件就无法更新它。管理程序还需要模拟可信执行环境（TEE），例如系统管理模式（SMM），并只允许在SMM环境中更新闪存。见图13-19。

<div align=center><img src=Figures/Chapter-13-Screenshot/Figure-13-19.jpg></img></div>
<div align=center>图 13-19 来自管理程序的安全启动</div>

但是，如果客户机域不信任管理程序，客户机域可能会将安全启动策略存储在只读闪存设备中。在这种情况下，安全启动解决方案需要与可信启动协同工作。当SOC启动客户机域时，SOC会测量虚拟固件中的代码和虚拟闪存配置区中的策略。必须注意的是，如果可信执行环境（TEE）是由管理程序模拟的，则客户机域不能信任TEE。只有当TEE是由SOC提供时，客户机域的TEE才能被信任。见图13-20。

<div align=center><img src=Figures/Chapter-13-Screenshot/Figure-13-20.jpg></img></div>
<div align=center>图 13-19 来自SOC的安全启动</div>

## 总结

在本章中，我们讨论了虚拟固件的安全设计，包括硬件安全扩展、设备接口支持以及在虚拟固件中实现的特殊安全功能。在第三部分，我们将介绍固件中的安全实现。

## 参考

**书**

[B-1] Jim Smith, Ravi Nair, Virtual Machines: Versatile Platforms for Systems and Processes, Morgan Kaufmann, 2005

[B-2] Edouard Bugnion, Jason Nieh, Dan Tsafrir, Hardware and Software Support for Virtualization, Morgan & Claypool Publishers, 2017

**会议、期刊与论文**

[P-1] J. P. Anderson, “Computer security technology planning study,” in Technical Report ESD-TR-73-51, vols I & II, AD-758 206, USAF Electronic Systems Division. 1972.

[P-2] Gerald J. Popek, Robert P. Goldberg, “Formal Requirements for Virtualizable Third Generation Architectures,” in ACM SIGOPS Operating Systems Review 17:412-421, January 1974.

[P-3] Robert P. Goldberg, “Survey of Virtual Machines Research,” in IEEE Computer, Volume: 7, Issue: 6, June 1974.

[P-4] David Kaplan, Jeremy Powell, Tom Woller, “AMD Memory Encryption,” AMD Whitepaper 2016, available at https://www.amd.com/content/dam/amd/en/documents/epyc-business-docs/white-papers/memory-encryption-white-paper.pdf

[P-5] David Kaplan, “Protecting VM Register State with SEV-ES,” AMD Whitepaper 2017, available at https://www.amd.com/content/dam/amd/en/documents/epyc-business-docs/white-papers/Protecting-VM-Register-State-with-SEV-ES.pdf

[P-6] David Kaplan, “AMD x86 Memory Encryption Technologies,” in 25th Usenix Security Symposium 2016, available at https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/kaplan

[P-7] Kai Huang, “Protect Data of Virtual Machines with MKTME on KVM,” in KVM Forum 2018, available at https://www.linux-kvm.org/images/d/d7/Mktme_kvm_forum_2018.pdf

[P-8] “Intel Trust Domain Extensions,” Intel whitepaper 2020, available at https://www.intel.com/content/dam/develop/external/us/en/documents/tdx-whitepaper-final9-17.pdf [注1]

[P-9] Guerney D. H. Hunt, “Protected Execution Facility – Secure computing for Linux on OpenPOWER,” in 2018, https://events19.linuxfoundation.org/wp-content/uploads/2017/12/Protected-Execution-Facility-Guerney-D.-H.-Hunt-IBM-Research.pdf

[P-10] Guerney Hunt, Richard (Rick) Boivie, Eric Hall, Elaine Palmer, Dimitrios Pendarakis, Enriquillo (Ray) Valdez, “Supporting protected computing on IBM Power
Architecture,” in IBM whitepaper 2018, available at https://developer.ibm.com/articles/l-support-protected-computing/ [注2]

[P-11] Rodrigo Branco, Shay Gueron, “Blinded random corruption attacks,” in IEEE International Symposium on Hardware Oriented Security and Trust (HOST), 2016

[P-12] Mathias Morbitzer, Manual Huber, Julian Horsch, Sascha Wessel, “SEVered: Subverting AMD’s Virtual Machine Encryption,” in Proceedings of the 11th European
Workshop on Systems Security, 2018

[P-13] “AMD-SEV: Platform DH key recovery via invalid curve attack,” 2019, available at https://seclists.org/fulldisclosure/2019/Jun/46

[P-14] Roman Kagan, “VMBus (Hyper-V) devices in QEMU/KVM,” in Linux Foundation Event 2017, available at http://events17.linuxfoundation.org/sites/events/files/slides/VMBus%20%28Hyper-V%29%20devices%20in%20QEMU%252FKVM_0.pdf

[P-15] Alex Ionescu, “Ring 0 to Ring -1 Attacks – Hyper-V IPC Internals,” in SYSCAN 2015, available at www.alex-ionescu.com/syscan2015.pdf [注2]

[P-16] Andrea Allievi, “Hyper-V and its Memory Manager,” in Recon 2017, available at http://www.andrea-allievi.com/files/Recon_2017_Montreal_HyperV_public.pptx

[P-17] Joe Bialek, Nicolas Joly, “A Dive in to Hyper-V Architecture & Vulnerabilities,” in Blackhat US 2018, available at http://i.blackhat.com/us-18/Wed-August-8/us-18-Joly-Bialek-A-Dive-in-to-Hyper-V-Architecture-and-Vulnerabilities.pdf

[P-18] Somnath Chakrabarti, Brandon Baker, Mona Vij, “Intel® SGX Enabled Key Manager Service with OpenStack Barbican,” 2017, https://arxiv.org/ftp/arxiv/papers/1712/1712.07694.pdf

[P-19] Jordan Rabet, “Hardening Hyper-V through Offensive Security Research,” in Blackhat US 2018, available at http://i.blackhat.com/us-18/Thu-August-9/us-18-Rabet-Hardening-Hyper-V-Through-Offensive-Security-Research.pdf

[P-20] Stefan Berger, Ramon Caceres, Kenneth A. Goldman, Ronald Perez, Reiner Sailer, Leendert van Doorn, “vTPM: Virtualizing the Trusted Platform Module,” in
15th Usenix Security Symposium, available at https://www.usenix.org/legacy/event/sec06/tech/full_papers/berger/berger.pdf

[P-21] Joshua Schiffman, “vTPM: Virtualizing the Trusted Platform Module,” in Systems and Internet Infrastructure Security, https://www.usenix.org/legacy/event/sec06/tech/full_papers/berger/berger.pdf [注1]

**规范和指南**

[S-1] AMD, “AMD Architecture Programmer’s Manual,” 2019, available at https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/programmer-references/40332.pdf [注1]

[S-2] AMD, “Secure Encrypted Virtualization API,” 2019, available at https://www.amd.com/content/dam/amd/en/documents/epyc-technical-docs/programmer-references/55766_SEV-KM_API_Specification.pdf [注1]

[S-3] AMD, “Guest Hypervisor Communication Block (GHCB) Standardization,” 2019, available at https://www.amd.com/content/dam/amd/en/documents/epyc-technical-docs/specifications/56421.pdf [注1]

[S-4] AMD, “SEV Secure Nested Paging Firmware ABI Specification,” 2020, available at https://www.amd.com/content/dam/amd/en/documents/epyc-technical-docs/specifications/56860.pdf [注1]

[S-5] Intel, “Intel 64 and IA-32 Architecture Software Developer Manuals,” 2019, available at https://software.intel.com/en-us/articles/intel-sdm

[S-6] Intel, “Intel Architecture Memory Encryption Technologies Specification,” 2019, available at https://software.intel.com/sites/default/files/managed/a5/16/Multi-Key-Total-Memory-Encryption-Spec.pdf

[S-7] Intel, “Intel Scalable I/O Virtualization Specification,” 2018, available at https://cdrdv2-public.intel.com/671403/intel-scalable-io-virtualization-technical-specification.pdf [注1]

[S-8] Intel, “Virtualization Technology for Directed I/O specification,” 2019, available at https://cdrdv2.intel.com/v1/dl/getContent/774206?fileName=vt-directed-io-spec+.pdf [注1]

[S-9] OASIS, “Virtual I/O Device (VIRTIO) Version 1.1,” 2019, available at http://docs.oasis-open.org/virtio/virtio/v1.1/virtio-v1.1.html

[S-10] Microsoft, “Hypervisor Specification,” 2018, https://docs.microsoft.com/en-us/virtualization/hyper-v-on-windows/reference/tlfs

[S-11] Trusted Computing Group, “Virtualized Trusted Platform Architecture Specification,” 2011, available at https://trustedcomputinggroup.org/resource/virtualized-trusted-platform-architecture-specification/

[S-12] PCI-SIG, “PCI Express Base Specification,” 2019, available at https://pcisig.com/specifications

[S-13] CXL org, “The CXL Specification,” 2019, available at https://computeexpresslink.org/

**网页**

[W-1] Amit Singh, “An Introduction to Virtualization,” 2004, www.kernelthread.com/publications/virtualization/

[W-2] “AMD Secure Encrypted Virtualization,” https://developer.amd.com/sev/

[W-3] “Windows VirtIo Drivers,” https://www.linux-kvm.org/page/WindowsGuestDrivers/Download_Drivers

[W-4] Microsoft, “Hyper-V Architecture,” 2018, https://learn.microsoft.com/en-us/windows-server/administration/performance-tuning/role/hyper-v-server/architecture [注1]

[W-5] Nirmal Sharma, “Understanding Hyper-V VSP/VSC and VMBUS Design,” 2013, https://www.serverwatch.com/guides/understanding-hyper-v-vsp-vsc-and-vmbus-design/ [注1]

[W-6] Gerhart, “Hyper-V Internals,” 2015, http://hvinternals.blogspot.com/2015/10/hyper-v-internals.html

[W-7] “Hyper-V / VMBus device emulation in Qemu,” https://bitbucket.org/openvz/edk2/src/master/ [注1]

[W-8] “Hyper-V enlightenments,” https://github.com/qemu/qemu/blob/master/docs/system/i386/hyperv.rst [注1]

[W-9] “TPM 2.0 Simulator for Linux/TEE,” https://develop.trustedcomputinggroup.org/2019/05/08/tpm-2-0-simulator-for-linux-tee/, https://github.com/stwagnr/tpm2simulator

[W-10] “TSS.MSR: The TPM Software Stack from Microsoft Research,” www.microsoft.com/en-us/download/details.aspx?id=52507, https://github.com/microsoft/TSS.MSR

[W-11] “IBM’s Software TPM 2.0,” https://sourceforge.net/projects/ibmswtpm2/

[W-12] “QEMU TPM Device,” https://github.com/qemu/qemu/blob/master/docs/specs/tpm.rst [注1]

[W-13] Miriam Zimmerman, “Virtual Trusted Platform Module for Shielded VMs: security in plaintext,” 2018, https://cloud.google.com/blog/products/identity-security/virtual-trusted-platform-module-for-shielded-vms-security-in-plaintext [注1]

[W-14] Jason Geffner, “VENOM – Virtualized Environment Neglected Operations Manipulation,” available at https://venom.crowdstrike.com/, https://www.crowdstrike.com/blog/venom-vulnerability-details/

[W-15] Vishnu Dev, “QEMU VM escape,” 2019, available at https://blog.bi0s.in/2019/08/24/Pwn/VM-Escape/2019-07-29-qemu-vm-escape-cve-2019-14378/,
https://www.secpod.com/blog/qemu-vm-escape/

[W-16] Tencent Blade Team, “V-gHost : QEMU-KVM VM Escape in vhost/vhost-net,” available at https://tuxcare.com/blog/qemu-kvm-vhost-vhost_net-guest-to-host-kernel-escape-vulnerability/  [注1]

 [注1] 原链接失效，找到了新的链接

 [注2] 原链接失效，未找到替代链接